{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE 7300: Statistical learning for Engineering\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Homework -7\n",
      "Student name :Xinan Wang\n",
      "Student Email :wang.xina@northeastern.edu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Percentage of Effort Contributed by Student : 100%\n",
      "Submission Date: 2022-11-01\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "studentName=\"Xinan Wang\"\n",
    "studentEmail=\"wang.xina@northeastern.edu\"\n",
    "homework=7\n",
    "contributedPercentage=100\n",
    "print(\"IE 7300: Statistical learning for Engineering\")\n",
    "print(\"\\n\"*15)\n",
    "print(f'Homework -{homework}')\n",
    "print(f'Student name :{studentName}')\n",
    "print(f'Student Email :{studentEmail}')\n",
    "print(\"\\n\"*15)\n",
    "print(f'Percentage of Effort Contributed by Student : {contributedPercentage}%')\n",
    "print(f'Submission Date: {date.today()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports ##\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Custom decision tree regressor\n",
    "from statmodels.decisiontrees import decisiontreeregressor\n",
    "#Custom random forest regressor\n",
    "from statmodels.random_forest import RandomForestRegressor\n",
    "#Custom lasso regressor\n",
    "from statmodels.regression import LassoRegression\n",
    "#Custom gradient boosting regressor\n",
    "from statmodels.gradientboosting import GradientBoostTreeRegressor\n",
    "\n",
    "#Find performance\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appliances energy prediction Dataset - Regression\n",
    "\n",
    "We can now look at a regression problem, the appliances energy prediction dataset. Let's import these data into the notebook, and do some exploratory analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#load the boston dataset\n",
    "data = pd.read_csv('energydata_complete.csv')\n",
    "#obtain input data\n",
    "X = data.drop(['Appliances','date'],axis=1).values\n",
    "#obtain labels\n",
    "Y = data['Appliances'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>T5</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>47.596667</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>45.566667</td>\n",
       "      <td>17.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.033333</td>\n",
       "      <td>45.5300</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>733.5</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>13.275433</td>\n",
       "      <td>13.275433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.693333</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.722500</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>45.992500</td>\n",
       "      <td>17.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>45.5600</td>\n",
       "      <td>6.483333</td>\n",
       "      <td>733.6</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>59.166667</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>18.606195</td>\n",
       "      <td>18.606195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.626667</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.926667</td>\n",
       "      <td>45.890000</td>\n",
       "      <td>17.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.5000</td>\n",
       "      <td>6.366667</td>\n",
       "      <td>733.7</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>28.642668</td>\n",
       "      <td>28.642668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.066667</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.590000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.723333</td>\n",
       "      <td>17.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.4000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>733.8</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>45.410389</td>\n",
       "      <td>45.410389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>44.530000</td>\n",
       "      <td>19.790000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.530000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.4000</td>\n",
       "      <td>6.133333</td>\n",
       "      <td>733.9</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>10.084097</td>\n",
       "      <td>10.084097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19730</th>\n",
       "      <td>0</td>\n",
       "      <td>25.566667</td>\n",
       "      <td>46.560000</td>\n",
       "      <td>25.890000</td>\n",
       "      <td>42.025714</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>41.163333</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.733333</td>\n",
       "      <td>755.2</td>\n",
       "      <td>55.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>23.666667</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>43.096812</td>\n",
       "      <td>43.096812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19731</th>\n",
       "      <td>0</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>25.754000</td>\n",
       "      <td>42.080000</td>\n",
       "      <td>27.133333</td>\n",
       "      <td>41.223333</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>23.230000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.600000</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>49.282940</td>\n",
       "      <td>49.282940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19732</th>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.596667</td>\n",
       "      <td>25.628571</td>\n",
       "      <td>42.768571</td>\n",
       "      <td>27.050000</td>\n",
       "      <td>41.690000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.730000</td>\n",
       "      <td>23.230000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.7900</td>\n",
       "      <td>22.466667</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>13.266667</td>\n",
       "      <td>29.199117</td>\n",
       "      <td>29.199117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19733</th>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.990000</td>\n",
       "      <td>25.414000</td>\n",
       "      <td>43.036000</td>\n",
       "      <td>26.890000</td>\n",
       "      <td>41.290000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.790000</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.8175</td>\n",
       "      <td>22.333333</td>\n",
       "      <td>755.2</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>26.166667</td>\n",
       "      <td>13.233333</td>\n",
       "      <td>6.322784</td>\n",
       "      <td>6.322784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19734</th>\n",
       "      <td>10</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>46.600000</td>\n",
       "      <td>25.264286</td>\n",
       "      <td>42.971429</td>\n",
       "      <td>26.823333</td>\n",
       "      <td>41.156667</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>45.963333</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>46.8450</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>755.2</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>34.118851</td>\n",
       "      <td>34.118851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19735 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lights         T1       RH_1         T2       RH_2         T3  \\\n",
       "0          30  19.890000  47.596667  19.200000  44.790000  19.790000   \n",
       "1          30  19.890000  46.693333  19.200000  44.722500  19.790000   \n",
       "2          30  19.890000  46.300000  19.200000  44.626667  19.790000   \n",
       "3          40  19.890000  46.066667  19.200000  44.590000  19.790000   \n",
       "4          40  19.890000  46.333333  19.200000  44.530000  19.790000   \n",
       "...       ...        ...        ...        ...        ...        ...   \n",
       "19730       0  25.566667  46.560000  25.890000  42.025714  27.200000   \n",
       "19731       0  25.500000  46.500000  25.754000  42.080000  27.133333   \n",
       "19732      10  25.500000  46.596667  25.628571  42.768571  27.050000   \n",
       "19733      10  25.500000  46.990000  25.414000  43.036000  26.890000   \n",
       "19734      10  25.500000  46.600000  25.264286  42.971429  26.823333   \n",
       "\n",
       "            RH_3         T4       RH_4         T5  ...         T9     RH_9  \\\n",
       "0      44.730000  19.000000  45.566667  17.166667  ...  17.033333  45.5300   \n",
       "1      44.790000  19.000000  45.992500  17.166667  ...  17.066667  45.5600   \n",
       "2      44.933333  18.926667  45.890000  17.166667  ...  17.000000  45.5000   \n",
       "3      45.000000  18.890000  45.723333  17.166667  ...  17.000000  45.4000   \n",
       "4      45.000000  18.890000  45.530000  17.200000  ...  17.000000  45.4000   \n",
       "...          ...        ...        ...        ...  ...        ...      ...   \n",
       "19730  41.163333  24.700000  45.590000  23.200000  ...  23.200000  46.7900   \n",
       "19731  41.223333  24.700000  45.590000  23.230000  ...  23.200000  46.7900   \n",
       "19732  41.690000  24.700000  45.730000  23.230000  ...  23.200000  46.7900   \n",
       "19733  41.290000  24.700000  45.790000  23.200000  ...  23.200000  46.8175   \n",
       "19734  41.156667  24.700000  45.963333  23.200000  ...  23.200000  46.8450   \n",
       "\n",
       "           T_out  Press_mm_hg     RH_out  Windspeed  Visibility  Tdewpoint  \\\n",
       "0       6.600000        733.5  92.000000   7.000000   63.000000   5.300000   \n",
       "1       6.483333        733.6  92.000000   6.666667   59.166667   5.200000   \n",
       "2       6.366667        733.7  92.000000   6.333333   55.333333   5.100000   \n",
       "3       6.250000        733.8  92.000000   6.000000   51.500000   5.000000   \n",
       "4       6.133333        733.9  92.000000   5.666667   47.666667   4.900000   \n",
       "...          ...          ...        ...        ...         ...        ...   \n",
       "19730  22.733333        755.2  55.666667   3.333333   23.666667  13.333333   \n",
       "19731  22.600000        755.2  56.000000   3.500000   24.500000  13.300000   \n",
       "19732  22.466667        755.2  56.333333   3.666667   25.333333  13.266667   \n",
       "19733  22.333333        755.2  56.666667   3.833333   26.166667  13.233333   \n",
       "19734  22.200000        755.2  57.000000   4.000000   27.000000  13.200000   \n",
       "\n",
       "             rv1        rv2  \n",
       "0      13.275433  13.275433  \n",
       "1      18.606195  18.606195  \n",
       "2      28.642668  28.642668  \n",
       "3      45.410389  45.410389  \n",
       "4      10.084097  10.084097  \n",
       "...          ...        ...  \n",
       "19730  43.096812  43.096812  \n",
       "19731  49.282940  49.282940  \n",
       "19732  29.199117  29.199117  \n",
       "19733   6.322784   6.322784  \n",
       "19734  34.118851  34.118851  \n",
       "\n",
       "[19735 rows x 27 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfX = data.drop(['Appliances','date'],axis=1)\n",
    "dfX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(dfX, Y, test_size = 0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Create a Lasso regression, decision tree, Random forest, and GradientBoost models. Fit the model using the training dataset and find the model RMSE and R-Square. Explain each model's outcome, finding, and accuracy. (4x3=12 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from statmodels.random_forest.abstract.base_randomforest import RandomForest\n",
    "from statmodels.decisiontrees import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#class for random forest regressor\n",
    "class RandomForestRegressor(RandomForest):\n",
    "    #initializer\n",
    "    def __init__(self,n_trees=10,max_depth=None,min_samples_split=2,loss='mse'):\n",
    "        super().__init__(n_trees)\n",
    "        self.max_depth             = max_depth\n",
    "        self.min_samples_split     = min_samples_split\n",
    "        self.loss                  = loss\n",
    "        \n",
    "    #protected function to obtain the right decision tree\n",
    "    def _make_tree_model(self):\n",
    "        return(DecisionTreeRegressor(self.max_depth,self.min_samples_split,self.loss))\n",
    "    \n",
    "    #public function to return model parameters\n",
    "    def get_params(self, deep = False):\n",
    "        return {'n_trees':self.n_trees,\n",
    "                'max_depth':self.max_depth,\n",
    "                'min_samples_split':self.min_samples_split,\n",
    "                'loss':self.loss}\n",
    "    \n",
    "    #train the ensemble\n",
    "    def fit(self,X_train,y_train):\n",
    "        #call the protected training method\n",
    "        dcOob = self._train(X_train,y_train)\n",
    "            \n",
    "    #predict from the ensemble\n",
    "    def predict(self,X):\n",
    "        #call the protected prediction method\n",
    "        ypred = self._predict(X)\n",
    "        #return the results\n",
    "        return(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a random forest with default values\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## train the ensemble & view estimates for prediction error ##\n",
    "Ypred_train = rfr.predict(Xtrain.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Mean Squared error: 1014.4278\n",
      "Random Forest - R-Squared: 0.9046\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest - Mean Squared error:', round(mean_squared_error(Ytrain,Ypred_train),4))\n",
    "print('Random Forest - R-Squared:', round(r2_score(Ytrain,Ypred_train),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training dataset accuracy output for the random forest model, we can find that the mean squared error is 1014.4278 and the R - squared value is 0.9046. This means our model has 90.46% accuracy on the training dataset.This is a very high probability rate. This means our model performance is really perfect on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  b) Predict the models using the test dataset, and provide the performance metrics. Compare the four models' performance metrics, and explain at least four findings on each of the models. Do not repeat the code to fit the model. (4x3=12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train the ensemble & view estimates for prediction error ##\n",
    "Ypred = rfr.predict(Xtest.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Mean Squared error: 4877.3334\n",
      "Random Forest - R-Squared: 0.5126\n",
      "Random Forest - Mean Absolute error: 34.2881\n",
      "Random Forest - Explained variance score: 0.5127\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest - Mean Squared error:', round(mean_squared_error(Ytest,Ypred),4))\n",
    "print('Random Forest - R-Squared:', round(r2_score(Ytest,Ypred),4))\n",
    "print('Random Forest - Mean Absolute error:', round(mean_absolute_error(Ytest,Ypred),4))\n",
    "print('Random Forest - Explained variance score:',round(explained_variance_score(Ytest,Ypred),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:\n",
    "\n",
    "From the testing dataset accuracy output for the random forest model, we can find that:\n",
    "\n",
    "• The mean squared error is 4877.3334, which is kind of large. This means our mean squared error regression loss is 4877.3334.\n",
    "\n",
    "• R - squared value is 0.5126. This means we have 51.26% accurancy on the testing dataset. This is a low rate. This means our model performance is not working very well on the testing dataset because the accuracy is just a little bit above 50%. Compared to the training dataset which has 90.46% accuracy, our testing dataset accuracy is extremely low. Therefore, our model has a significant overfitting because the training dataset accuracy is greatly higher than the testing dataset accuracy.\n",
    "\n",
    "• The mean absolute error is 34.2881, which means mean absolute error regression loss of this model is 34.2881. \n",
    "\n",
    "• The explained variance regression score of this function is 0.5127."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  c) Do you see any bias and variance issues? How do you interpret each model output? (4x3=12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Random Forest model, we can see that for the training dataset, the MSE is 1014.4278, which is significanly less than the MSE of the testing dataset which is 4877.3334. And the R-squared for the predicting training dataset is 0.9046. It's also significantly greater than the testing dataset R-squared which is 0.5126. This means the performance of the random forest model works well on the training dataset and it performs bad in the testing dataset. Therefore, we have an overfitting issue and thus we have a <b>high variance </b> in our model. Variance indicates how much the estimate of the target function will alter if different training data were used. And it measures the inconsistency of different predictions using different training sets. Because that our training dataset works well, but our testing dataset works bad, we have a greatly inconsistency of different predictions using different sets. Therefore, we have a high variance in this model.\n",
    "\n",
    "Bias means the amount that a model's prediction differs from the target value, compared to the training data. Because that our model overall accuracy (90.46% in training dataset) works perfect, we could say we have a <b>low bias</b>.\n",
    "\n",
    "Therefore, we have a high variance and low bias in the random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Write a function to find important features in each model? Why is it an important feature of the model? Explain with some statistical evidence. (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_after_permutation(model, X, y, curr_feat):\n",
    "    \"\"\" return the score of model when curr_feat is permuted \"\"\"\n",
    "\n",
    "    X_permuted = X.copy()\n",
    "    col_idx = list(X.columns).index(curr_feat)\n",
    "    # permute one column\n",
    "    X_permuted.iloc[:, col_idx] = np.random.permutation(\n",
    "        X_permuted[curr_feat].values)\n",
    "    y_pred = model.predict(X_permuted.values)\n",
    "\n",
    "    permuted_score = r2_score(y, y_pred)\n",
    "    return permuted_score\n",
    "\n",
    "\n",
    "def get_feature_importance(model, X, y, curr_feat):\n",
    "    \"\"\" compare the score when curr_feat is permuted \"\"\"\n",
    "    y_pred = model.predict(X.values)\n",
    "\n",
    "    baseline_score_train = r2_score(y, y_pred)\n",
    "    permuted_score_train = get_score_after_permutation(model, X, y, curr_feat)\n",
    "\n",
    "    # feature importance is the difference between the two scores\n",
    "    feature_importance = baseline_score_train - permuted_score_train\n",
    "    return feature_importance\n",
    "    \n",
    "def permutation_importance(model, X, y, n_repeats=10):\n",
    "    \"\"\"Calculate importance score for each feature.\"\"\"\n",
    "\n",
    "    importances = []\n",
    "    for curr_feat in X.columns:\n",
    "        list_feature_importance = []\n",
    "        for n_round in range(n_repeats):\n",
    "            list_feature_importance.append(\n",
    "                get_feature_importance(model, X, y, curr_feat))\n",
    "\n",
    "        importances.append(list_feature_importance)\n",
    "\n",
    "    return {'importances_mean': np.mean(importances, axis=1),\n",
    "            'importances_std': np.std(importances, axis=1),\n",
    "            'importances': importances}\n",
    "\n",
    "def plot_feature_importances(perm_importance_result, feat_name):\n",
    "    \"\"\" bar plot the feature importance \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    indices = perm_importance_result['importances_mean'].argsort()\n",
    "    plt.barh(range(len(indices)),\n",
    "             perm_importance_result['importances_mean'][indices],\n",
    "             xerr=perm_importance_result['importances_std'][indices])\n",
    "\n",
    "    ax.set_yticks(range(len(indices)))\n",
    "    _ = ax.set_yticklabels(feat_name[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "perm_importance_result_train = permutation_importance(rfr, Xtrain, Ytrain, n_repeats=10)\n",
    "plot_feature_importances(perm_importance_result_train, Xtrain.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the feature importance output, we can find that the important variable in the random forest model is RH_8, Lights, RH_6, RH_1, RH_2 and RH_4. Because they takes a good portition in the importance feature chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  a) Create a Lasso regression, decision tree, Random forest, and GradientBoost models. Fit the model using the training dataset and find the model RMSE and R-Square. Explain each model's outcome, finding, and accuracy. (4x3=12 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, var_red=None, value=None):\n",
    "        ''' constructor ''' \n",
    "        \n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.var_red = var_red\n",
    "        # for leaf node\n",
    "        self.value = value\n",
    "\n",
    "class decisiontreeregressor():\n",
    "    \n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        ''' constructor '''\n",
    "        \n",
    "        # initialize the root of the tree \n",
    "        self.root = None\n",
    "        \n",
    "        # stopping conditions\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        #features\n",
    "        self.features_used = []\n",
    "        \n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        ''' recursive function to build the tree '''\n",
    "        \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        best_split = {}\n",
    "        # split until stopping conditions are met\n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            # find the best split\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"var_red\"]>0:\n",
    "                # recur left\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                # recur right\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                # generate feature list\n",
    "                self.features_used.append([self.features[best_split[\"feature_index\"]],curr_depth])\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"var_red\"])\n",
    "        \n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        # return leaf node\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        ''' function to find the best split '''\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        max_var_red = -float(\"inf\")\n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # check if childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # compute information gain\n",
    "                    curr_var_red = self.variance_reduction(y, left_y, right_y)\n",
    "                    # update the best split if needed\n",
    "                    if curr_var_red>max_var_red:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"var_red\"] = curr_var_red\n",
    "                        max_var_red = curr_var_red\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        ''' function to split the data '''\n",
    "        \n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    def variance_reduction(self, parent, l_child, r_child):\n",
    "        ''' function to compute variance reduction '''\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        reduction = np.var(parent) - (weight_l * np.var(l_child) + weight_r * np.var(r_child))\n",
    "        return reduction\n",
    "    \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        ''' function to compute leaf node '''\n",
    "        \n",
    "        val = np.mean(Y)\n",
    "        return val\n",
    "                \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' function to print the tree '''\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(round(tree.value,4))\n",
    "\n",
    "        else:\n",
    "            print(\"|-\",self.features[tree.feature_index], \"<=\", tree.threshold, \":Variance Red\", round(tree.var_red,4))\n",
    "            print(\"%sleft-->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright-->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        ''' function to train the tree '''\n",
    "        \n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.features = X.columns\n",
    "        self.root = self.build_tree(dataset)\n",
    "        \n",
    "    def make_prediction(self, x, tree):\n",
    "        ''' function to predict new dataset '''\n",
    "        \n",
    "        if tree.value!=None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' function to predict a single data point '''\n",
    "        \n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions\n",
    "    \n",
    "    def get_features(self):\n",
    "        return self.features_used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtModel = decisiontreeregressor(min_samples_split=3, max_depth=3)\n",
    "dtModel.fit(Xtrain,Ytrain.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypred_train = dtModel.predict(Xtrain.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Mean Squared error: 9078.794\n",
      "Decision Tree - R-Squared: 0.1464\n"
     ]
    }
   ],
   "source": [
    "print('Decision Tree - Mean Squared error:', round(mean_squared_error(Ytrain,Ypred_train),4))\n",
    "print('Decision Tree - R-Squared:', round(r2_score(Ytrain,Ypred_train),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training dataset accuracy output for the decision tree model, we can find that the mean squared error is 9078.794 and the R - squared value is 0.1464. This means we have 14.64% accurancy on the testing dataset. This is a low rate. This means our model performance is not working very well on the training dataset because the accuracy is greatly less than 60%. Our model has a poor fit on training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  b) Predict the models using the test dataset, and provide the performance metrics. Compare the four models' performance metrics, and explain at least four findings on each of the models. Do not repeat the code to fit the model. (4x3=12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypred = dtModel.predict(Xtest.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Mean Squared error: 9099.1746\n",
      "Decision Tree - R-Squared: 0.0907\n",
      "Decision Tree - Mean Absolute error: 54.0024\n",
      "Decision Tree - Explained variance score: 0.0911\n"
     ]
    }
   ],
   "source": [
    "print('Decision Tree - Mean Squared error:', round(mean_squared_error(Ytest,Ypred),4))\n",
    "print('Decision Tree - R-Squared:', round(r2_score(Ytest,Ypred),4))\n",
    "print('Decision Tree - Mean Absolute error:', round(mean_absolute_error(Ytest,Ypred),4))\n",
    "print('Decision Tree - Explained variance score:',round(explained_variance_score(Ytest,Ypred),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:\n",
    "\n",
    "From the testing dataset accuracy output for the decision tree model, we can find that:\n",
    "\n",
    "• The mean squared error is 9099.1746, which is kind of large. This means our mean squared error regression loss is 9099.1746.\n",
    "\n",
    "• R - squared value is 0.0907. This means we have 9.07% accurancy on the testing dataset. This is a low rate. This means our model performance is not working very well on the testing dataset because the accuracy is greating less than 60%. Compared to the training dataset which has 14.64% accuracy, even though the training dataset is low enough, our testing dataset accuracy is much lower than the model on the training dataset. Therefore, our model has a underfitting issue because the accuracy are very low for both of the training dataset and the testing dataset.\n",
    "\n",
    "• The mean absolute error is 54.0024, which means mean absolute error regression loss of this model is 54.0024, kind of large.\n",
    "\n",
    "• The explained variance regression score of this function is 0.0911, which is very small. Our model is not good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  c) Do you see any bias and variance issues? How do you interpret each model output? (4x3=12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Decision Tree model, we can see that for the training dataset, the RMSE is 9078.794, which has not much difference compared with the RMSE of the testing dataset which is 9099.1746. And the R-squared for the predicting training dataset is 0.1464. It's also not much difference compared with the testing dataset R-squared which is 0.0907. However, both of the high RMSE and the low R-Squared data of training dataset and the testing dataset indicates that our model has a bad performance on predicting the dataset. \n",
    "\n",
    "Therefore, we have a <b>low variance </b> in our model. Variance indicates how much the estimate of the target function will alter if different training data were used. And it measures the inconsistency of different predictions using different training sets. Because that both of our training dataset predictions and the testing dataset predictions perform bad, and they have very less difference on performance compared with each other, we can say that they have low variance because the model didn't change too much when using the different datasets.\n",
    "\n",
    "Bias means the amount that a model's prediction differs from the target value, compared to the training data. Because that our model RMSE is very high and the R-squared rate is very low, our model performs very bad. We could say we have a very <b>high bias</b>. This is because our model's prediction always very differ from the target value\n",
    "\n",
    "To conclude, we have a low variance and high bias in the random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Write a function to find important features in each model? Why is it an important feature of the model? Explain with some statistical evidence. (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>T5</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>19.89</td>\n",
       "      <td>47.596667</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>45.566667</td>\n",
       "      <td>17.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.033333</td>\n",
       "      <td>45.53</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>733.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>13.275433</td>\n",
       "      <td>13.275433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.693333</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.722500</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>45.992500</td>\n",
       "      <td>17.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>45.56</td>\n",
       "      <td>6.483333</td>\n",
       "      <td>733.6</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>59.166667</td>\n",
       "      <td>5.2</td>\n",
       "      <td>18.606195</td>\n",
       "      <td>18.606195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.626667</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.926667</td>\n",
       "      <td>45.890000</td>\n",
       "      <td>17.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.50</td>\n",
       "      <td>6.366667</td>\n",
       "      <td>733.7</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>5.1</td>\n",
       "      <td>28.642668</td>\n",
       "      <td>28.642668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.066667</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.590000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.723333</td>\n",
       "      <td>17.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.40</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>733.8</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>45.410389</td>\n",
       "      <td>45.410389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.530000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>45.530000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.40</td>\n",
       "      <td>6.133333</td>\n",
       "      <td>733.9</td>\n",
       "      <td>92.0</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>4.9</td>\n",
       "      <td>10.084097</td>\n",
       "      <td>10.084097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lights     T1       RH_1    T2       RH_2     T3       RH_3         T4  \\\n",
       "0      30  19.89  47.596667  19.2  44.790000  19.79  44.730000  19.000000   \n",
       "1      30  19.89  46.693333  19.2  44.722500  19.79  44.790000  19.000000   \n",
       "2      30  19.89  46.300000  19.2  44.626667  19.79  44.933333  18.926667   \n",
       "3      40  19.89  46.066667  19.2  44.590000  19.79  45.000000  18.890000   \n",
       "4      40  19.89  46.333333  19.2  44.530000  19.79  45.000000  18.890000   \n",
       "\n",
       "        RH_4         T5  ...         T9   RH_9     T_out  Press_mm_hg  RH_out  \\\n",
       "0  45.566667  17.166667  ...  17.033333  45.53  6.600000        733.5    92.0   \n",
       "1  45.992500  17.166667  ...  17.066667  45.56  6.483333        733.6    92.0   \n",
       "2  45.890000  17.166667  ...  17.000000  45.50  6.366667        733.7    92.0   \n",
       "3  45.723333  17.166667  ...  17.000000  45.40  6.250000        733.8    92.0   \n",
       "4  45.530000  17.200000  ...  17.000000  45.40  6.133333        733.9    92.0   \n",
       "\n",
       "   Windspeed  Visibility  Tdewpoint        rv1        rv2  \n",
       "0   7.000000   63.000000        5.3  13.275433  13.275433  \n",
       "1   6.666667   59.166667        5.2  18.606195  18.606195  \n",
       "2   6.333333   55.333333        5.1  28.642668  28.642668  \n",
       "3   6.000000   51.500000        5.0  45.410389  45.410389  \n",
       "4   5.666667   47.666667        4.9  10.084097  10.084097  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfX.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|- lights <= 0.0 :Variance Red 405.5117\n",
      " left-->|- RH_out <= 70.3333333333333 :Variance Red 307.4177\n",
      "  left-->|- T3 <= 23.5 :Variance Red 479.9704\n",
      "    left-->|- RH_1 <= 46.7666666666667 :Variance Red 239.8035\n",
      "        left-->98.5672\n",
      "        right-->500.0\n",
      "    right-->|- T1 <= 23.1527142857143 :Variance Red 1978.5137\n",
      "        left-->226.129\n",
      "        right-->120.1591\n",
      "  right-->|- RH_8 <= 39.1528571428571 :Variance Red 151.4739\n",
      "    left-->|- RH_3 <= 39.9 :Variance Red 734.4509\n",
      "        left-->103.0057\n",
      "        right-->286.0714\n",
      "    right-->|- T2 <= 19.39 :Variance Red 118.5591\n",
      "        left-->60.5075\n",
      "        right-->82.2869\n",
      " right-->|- T9 <= 17.1 :Variance Red 504.8531\n",
      "  left-->|- RH_2 <= 34.09 :Variance Red 2486.905\n",
      "    left-->980.0\n",
      "    right-->|- RH_5 <= 50.988888888888894 :Variance Red 1683.5191\n",
      "        left-->129.2771\n",
      "        right-->216.4478\n",
      "  right-->|- RH_out <= 80.5 :Variance Red 528.6071\n",
      "    left-->|- Windspeed <= 5.8333333333333295 :Variance Red 517.7859\n",
      "        left-->137.5207\n",
      "        right-->187.7692\n",
      "    right-->|- RH_4 <= 38.3266666666667 :Variance Red 324.8123\n",
      "        left-->83.6416\n",
      "        right-->120.4159\n"
     ]
    }
   ],
   "source": [
    "dtModel.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of features used in the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>featurename</th>\n",
       "      <th>treelevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lights</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RH_out</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>T9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RH_8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RH_2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RH_out</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RH_1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RH_3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RH_5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Windspeed</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RH_4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   featurename  treelevel\n",
       "13      lights          0\n",
       "6       RH_out          1\n",
       "12          T9          1\n",
       "2           T3          2\n",
       "5         RH_8          2\n",
       "8         RH_2          2\n",
       "11      RH_out          2\n",
       "0         RH_1          3\n",
       "1           T1          3\n",
       "3         RH_3          3\n",
       "4           T2          3\n",
       "7         RH_5          3\n",
       "9    Windspeed          3\n",
       "10        RH_4          3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtFeatures = pd.DataFrame(dtModel.get_features())\n",
    "dtFeatures.columns = ['featurename','treelevel']\n",
    "dtFeatures = dtFeatures.drop_duplicates()\n",
    "dtFeatures.sort_values('treelevel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the printing decision tree and the list of features used in the decision tree, we found out that the RH_out variable appears twice and it's on treelevel 1 and 2, which are very close to the root node. Therefore, RH_out is the important feature in this model.\n",
    "\n",
    "ALso,lights variable is the root node. So it's an important variable as well.\n",
    "\n",
    "T9 variable is in the tree level 1 which is also near to the root variable so it's an important feature as well.\n",
    "\n",
    "<b>Therefore, in the decision tree model, the important features are 'RH_out', 'lights' and 'T9'.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  a) Create a Lasso regression, decision tree, Random forest, and GradientBoost models. Fit the model using the training dataset and find the model RMSE and R-Square. Explain each model's outcome, finding, and accuracy. (4x3=12 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Module containing classes for supervised linear regression models.\"\"\"\n",
    "import numpy as np\n",
    "from scipy.linalg import lstsq\n",
    "\n",
    "from statmodels.regression.utils.regularization import l1_regularization, l2_regularization,l1_l2_regularization\n",
    "from statmodels.regression.utils.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class Regression():\n",
    "    \"\"\"\n",
    "    Class representing our base regression model.  \n",
    "    \n",
    "    Models relationship between a dependant scaler variable y and independent\n",
    "    variables X by optimizing a cost function with batch gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_iter : float, default=1000\n",
    "        Maximum number of iterations to be used by batch gradient descent.\n",
    "    lr : float, default=1e-1\n",
    "        Learning rate determining the size of steps in batch gradient descent.\n",
    "\n",
    "    Attributes \n",
    "    ----------\n",
    "    coef_ : array of shape (n_features,)\n",
    "        Estimated coefficients for the regression problem.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_iter=1000, lr=1e-1):\n",
    "        self.n_iter = n_iter \n",
    "        self.lr = lr \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit linear model with batch gradient descent.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data. Independent variables.\n",
    "        y : array-like of shape (n_samples, 1)\n",
    "            Target values. Dependent variable.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        # Insert X_0 = 1 for the bias term.\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "        # Store number of samples and features in variables.\n",
    "        n_samples, n_features = np.shape(X)\n",
    "        self.training_errors = []\n",
    "\n",
    "        # Randomly intialize weights using glorot uniform intializer.\n",
    "        limit = np.sqrt(2 / n_features)\n",
    "        self.coef_ = np.random.uniform(-limit, limit, (n_features,))\n",
    "\n",
    "        # Batch gradient descent for number iterations = n_iter.\n",
    "        for _ in range(self.n_iter):\n",
    "            y_preds = X.dot(self.coef_)\n",
    "\n",
    "            # Penalty term if regularized (don't include bias term).\n",
    "            regularization = self.regularization(self.coef_[1:])\n",
    "\n",
    "            # Calculate mse + penalty term if regularized.\n",
    "            cost_function = mean_squared_error(y, y_preds) + regularization\n",
    "            self.training_errors.append(cost_function) \n",
    "\n",
    "            # Regularization term of gradients (don't include bias term).\n",
    "            gradient_reg = self.regularization.grad(self.coef_[1:])\n",
    "\n",
    "            # Gradients of loss function.\n",
    "            gradients = (2/n_samples) * X.T.dot(y_preds - y)\n",
    "            gradients = gradients + gradient_reg\n",
    "\n",
    "            # Update the weights.\n",
    "            self.coef_ -= self.lr * gradients \n",
    "\n",
    "        return self \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Estimate target values using the linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Instances.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array of shape (n_samples,)\n",
    "            Estimated targets per instance.\n",
    "        \"\"\"\n",
    "        # Insert X_0 = 1 for the bias term.\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "        return X.dot(self.coef_)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate the coefficient of determination, R^2 of the predictions.\n",
    "\n",
    "            R^2 = 1 - SS_res / SS_tot\n",
    "\n",
    "        where SS_res is the residual sum of squares and SS_tot is the total\n",
    "        sum of squares.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Test samples for model to be scores against.\n",
    "        y : array-like of shape (n_samples,).\n",
    "            True values for test samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            R^2 calculated on test samples.\n",
    "        \"\"\"\n",
    "        y_preds = self.predict(X)\n",
    "\n",
    "        score = r2_score(y, y_preds)\n",
    "\n",
    "        return score\n",
    "\n",
    "\n",
    "class LinearRegression(Regression):\n",
    "    \"\"\"\n",
    "    Class representing a linear regression model.\n",
    "\n",
    "    Models relationship between target variable and attributes by computing \n",
    "    line that minimizes mean squared error.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_iter : float, default=1000\n",
    "        Maximum number of iterations to be used by batch gradient descent.\n",
    "    lr : float, default=1e-1\n",
    "        Learning rate determining the size of steps in batch gradient descent.     \n",
    "    solver : {'bgd', 'lstsq'}, default=\"bgd\"\n",
    "        Optimization method used to minimize mean squared error in training.\n",
    "\n",
    "        'bgd' : \n",
    "            Batch gradient descent.\n",
    "\n",
    "        'lstsq' : \n",
    "            Ordinary lease squares method using scipy.linalg.lstsq.\n",
    "\n",
    "    Attributes \n",
    "    ----------\n",
    "    coef_ : array of shape (n_features,)\n",
    "        Estimated coefficients for the regression problem.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This class is capable of being trained using ordinary least squares method\n",
    "    or batch gradient descent.  See solver parameter above.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_iter=1000, lr=1e-1, solver='bgd'):\n",
    "        self.solver = solver \n",
    "\n",
    "        # No regularization.\n",
    "        self.regularization = lambda x: 0\n",
    "        self.regularization.grad = lambda x: 0\n",
    "        super(LinearRegression, self).__init__(n_iter=n_iter, lr=lr)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit linear regression model.\n",
    "\n",
    "        If solver='bgd', model is trained using batch gradient descent. \n",
    "        If solver='lstsq' model is trained using ordinary least squares.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data. Independent variables.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values. Dependent variable.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        # If solver is 'lstsq' use ordinary least squares optimization method.\n",
    "        if self.solver == 'lstsq':\n",
    "\n",
    "            # Insert X_0 = 1 for the bias term.\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            \n",
    "            # Scipy implementation of least squares.\n",
    "            self.coef_, residues, rank, singular = lstsq(X, y)\n",
    "\n",
    "            return self\n",
    "\n",
    "        elif self.solver == 'bgd': \n",
    "            super(LinearRegression, self).fit(X, y)\n",
    "\n",
    "\n",
    "class RidgeRegression(Regression):\n",
    "    \"\"\"\n",
    "    Class representing a linear regression model with l2 regularization.\n",
    "\n",
    "    Minimizes the cost fuction:\n",
    "\n",
    "        J(w) = MSE(w) + alpha * 1/2 * ||w||^2\n",
    "\n",
    "    where w is the vector of feature weights and alpha is the hyperparameter\n",
    "    controlling how much regularization is done to the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_iter : float, default=1000\n",
    "        Maximum number of iterations to be used by batch gradient descent.\n",
    "    lr : float, default=1e-1\n",
    "        Learning rate determining the size of steps in batch gradient descent.\n",
    "    alpha : float, default=1.0\n",
    "        Factor determining the amount of regularization to be performed on\n",
    "        the model.\n",
    "\n",
    "    Attributes \n",
    "    ----------\n",
    "    coef_ : array of shape (n_features,)\n",
    "        Estimated coefficients for the regression problem.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This class is capable of being trained using batch gradient descent at\n",
    "    current version.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_iter=1000, lr=1e-1, alpha=1.0, solver='bgd'):\n",
    "        self.alpha = alpha\n",
    "        self.regularization = l2_regularization(alpha=self.alpha)\n",
    "        super(RidgeRegression, self).__init__(n_iter=n_iter, lr=lr)\n",
    "\n",
    "\n",
    "class LassoRegression(Regression):\n",
    "    \"\"\"\n",
    "    Class representing a linear regression model with l1 regularization.\n",
    "\n",
    "    Minimizes the cost fuction:\n",
    "\n",
    "        J(w) = MSE(w) + alpha * ||w||\n",
    "\n",
    "    where w is the vector of feature weights and alpha is the hyperparameter\n",
    "    controlling how much regularization is done to the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_iter : float, default=1000\n",
    "        Maximum number of iterations to be used by batch gradient descent.\n",
    "    lr : float, default=1e-2\n",
    "        Learning rate determining the size of steps in batch gradient descent.\n",
    "    alpha : float, default=1.0\n",
    "        Factor determining the amount of regularization to be performed on\n",
    "        the model.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    coef_ : array of shape (n_features,)\n",
    "        Estimated coefficients for the regression problem.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This class is capable of being trained using batch gradient descent at\n",
    "    current version.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_iter=1000, lr=1e-2, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.regularization = l1_regularization(alpha=self.alpha)\n",
    "        super(LassoRegression, self).__init__(n_iter=n_iter, lr=lr)\n",
    "\n",
    "\n",
    "class ElasticNetRegression(Regression):\n",
    "    \"\"\"\n",
    "    Class representing a linear regression model with a mix of l1 and l2 \n",
    "    regularization.\n",
    "\n",
    "    Minimizes the cost function:\n",
    "\n",
    "        J(w) = MSE(w) + r * alpha * ||w|| + (1 - r) * alpha * 1/2 * ||w||^2\n",
    "\n",
    "    where w is the vector of feature weights, r is the mix ratio, and alpha\n",
    "    is the hyperparameter controlling how much regularization is done.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_iter : float, default=1000\n",
    "        Maximum number of iterations to be used by batch gradient descent.\n",
    "    lr : float, default=1e-2\n",
    "        Learning rate determining the size of steps in batch gradient descent.\n",
    "    alpha : float, default=1.0\n",
    "        Factor determining the amount of regularization to be performed on\n",
    "        the model.\n",
    "    r : float, default=0.5\n",
    "        Mix ratio determining the amount of l1 vs l2 regularization to add.  \n",
    "        A value of 0 is equivalent to l2 regularization and a value of 1 is\n",
    "        equivalent to l1 regularization.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    coef_ : array of shape (n_features,)\n",
    "        Estimated coefficients for the regression problem.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This class is capable of being trained using batch gradient descent at\n",
    "    current version.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_iter=1000, lr=1e-2, alpha=1.0, r=0.5):\n",
    "        self.alpha = alpha\n",
    "        self.r = r \n",
    "        self.regularization = l1_l2_regularization(alpha=self.alpha, r=self.r)\n",
    "        super(ElasticNetRegression, self).__init__(n_iter=n_iter, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "dfX_tranformed = pd.DataFrame(sc.fit_transform(dfX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(dfX_tranformed, Y, test_size = 0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LassoRegression at 0x7f917caaf340>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "lassoModel = LassoRegression(alpha=0.1)\n",
    "lassoModel.fit(Xtrain.values, Ytrain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the training dataset\n",
    "Ypred_train = lassoModel.predict(Xtrain.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression - Mean Squared error: 9004.7142\n",
      "Lasso Regression - R-Squared: 0.1534\n"
     ]
    }
   ],
   "source": [
    "# Find the performance\n",
    "print('Lasso Regression - Mean Squared error:', round(mean_squared_error(Ytrain,Ypred_train),4))\n",
    "print('Lasso Regression - R-Squared:', round(r2_score(Ytrain,Ypred_train),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training dataset accuracy output for the random forest model, we can find that the mean squared error is 9004.7142 and the R - squared value is 0.1534. This means we have 15.34% accurancy on the testing dataset. This is a low rate. This means our model performance is not working very well on the training dataset because the accuracy is less than 60%. Our model has a poor fit on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Predict the models using the test dataset, and provide the performance metrics. Compare the four models' performance metrics, and explain at least four findings on each of the models. Do not repeat the code to fit the model. (4x3=12 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the testing dataset\n",
    "Ypred = lassoModel.predict(Xtest.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression - Mean Squared error: 8393.4284\n",
      "Lasso Regression - R-Squared: 0.1613\n",
      "Lasso Regression - Mean Absolute error: 52.7347\n",
      "Lasso Regression - Explained variance score: 0.1618\n"
     ]
    }
   ],
   "source": [
    "# Find the performance\n",
    "print('Lasso Regression - Mean Squared error:', round(mean_squared_error(Ytest,Ypred),4))\n",
    "print('Lasso Regression - R-Squared:', round(r2_score(Ytest,Ypred),4))\n",
    "print('Lasso Regression - Mean Absolute error:', round(mean_absolute_error(Ytest,Ypred),4))\n",
    "print('Lasso Regression - Explained variance score:',round(explained_variance_score(Ytest,Ypred),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:\n",
    "\n",
    "From the testing dataset accuracy output for the decision tree model, we can find that:\n",
    "\n",
    "• The mean squared error is 8393.4284, which is kind of large. This means our mean squared error regression loss is 8393.4284.\n",
    "\n",
    "• R - squared value is 0.1613. This means we have 9.07% accurancy on the testing dataset. This is a low rate. This means we have 16.13% accurancy on the testing dataset. This means our model performance is not working very well on the testing dataset because the accuracy is greating less than 60%. Compared to the training dataset which has 15.43% accuracy, even though the training dataset is low enough, our testing dataset accuracy is much lower than the model on the training dataset. Therefore, our model has a underfitting issue because the accuracy are very low for both of the training dataset and the testing dataset.\n",
    "\n",
    "• The mean absolute error is 52.7347, which means mean absolute error regression loss of this model is 52.7347, kind of large.\n",
    "\n",
    "• The explained variance regression score of this function is 0.1618, which is very small. Our model is not good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  c) Do you see any bias and variance issues? How do you interpret each model output? (4x3=12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Lasso regression model, we can see that for the training dataset, the RMSE is 9004.7142, which has not much difference compared with the RMSE of the testing dataset which is 8393.4284. And the R-squared for the predicting training dataset is 0.1534. It's also not much difference compared with the testing dataset R-squared which is 0.1613. However, both of the high RMSE and the low R-Squared data of training dataset and the testing dataset indicates that our model has a bad performance on predicting the dataset. \n",
    "\n",
    "Therefore, we have a <b>low variance </b> in our model. Variance indicates how much the estimate of the target function will alter if different training data were used. And it measures the inconsistency of different predictions using different training sets. Because that both of our training dataset predictions and the testing dataset predictions perform bad, and they have very less difference on performance compared with each other, we can say that they have low variance because the model didn't change too much when using the different datasets.\n",
    "\n",
    "Bias means the amount that a model's prediction differs from the target value, compared to the training data. Because that our model RMSE is very high and the R-squared rate is very low, our model performs very bad. We could say we have a very <b>high bias</b>. This is because our model's prediction always very differ from the target value\n",
    "\n",
    "To conclude, we have a low variance and high bias in the Lasso regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Write a function to find important features in each model? Why is it an important feature of the model? Explain with some statistical evidence. (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso model cofficients : [ 9.81546703e+01  1.66561642e+01 -1.01294803e+01  3.89444173e+01\n",
      " -7.52632657e-01 -2.10674431e+01  4.14052566e+01  1.29172400e+01\n",
      " -1.23983388e+01 -2.89359358e+00 -9.10362734e+00  1.91108695e+00\n",
      "  1.40264946e+01  3.43906247e+00 -4.89086095e-01 -1.15448849e+01\n",
      "  8.94776234e+00 -2.09537949e+01 -1.50820788e+01 -5.83189436e+00\n",
      " -5.60927680e+00  2.49326839e-01 -1.22827829e+00  4.43571512e+00\n",
      "  1.71676070e+00 -3.49487357e+00  6.62542260e-04  5.38482660e-04]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso model cofficients :\",lassoModel.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance for Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoImportantfeatures = pd.DataFrame(data={'feature': dfX.columns})\n",
    "lassoImportantfeatures[\"importance\"] = pd.DataFrame(lassoModel.coef_[1:])\n",
    "lassoImportantfeatures = lassoImportantfeatures.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(x=lassoImportantfeatures['feature'], height=lassoImportantfeatures['importance'], color='#087E8B')\n",
    "plt.title('Feature importances - for lasso regresion')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the feature importance output, we can find that the 'T3', 'RH_1', 'lights', 'T6', 'RH_3', 'T8' variables have high feature importance and therefore, they are the importance features of the Lasso regression Model.\n",
    "\n",
    "<b>The importance features of Lasso Regression are: 'T3', 'RH_1', 'lights', 'T6', 'RH_3', 'T8'</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  a) Create a Lasso regression, decision tree, Random forest, and GradientBoost models. Fit the model using the training dataset and find the model RMSE and R-Square. Explain each model's outcome, finding, and accuracy. (4x3=12 points) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List\n",
    "from sklearn.base import clone\n",
    "\n",
    "## boosting regressor ##\n",
    "class GradientBoostTreeRegressor(object):\n",
    "    #initializer\n",
    "    def __init__(self,  n_elements : int = 100, learning_rate : float = 0.01) -> None:\n",
    "        self.weak_learner  = DecisionTreeRegressor(max_depth=5)\n",
    "        self.n_elements    = n_elements\n",
    "        self.learning_rate = learning_rate\n",
    "        self.f             = []\n",
    "        self.residuals     = []\n",
    "        \n",
    "    #destructor\n",
    "    def __del__(self) -> None:\n",
    "        del self.weak_learner\n",
    "        del self.n_elements\n",
    "        del self.learning_rate\n",
    "        del self.f\n",
    "        del self.residuals\n",
    "    \n",
    "    #public function to return model parameters\n",
    "    def get_params(self, deep : bool = False) -> Dict:\n",
    "        return {'weak_learner':self.weak_learner,'n_elements':self.n_elements,'learning_rate':self.learning_rate}\n",
    "    \n",
    "    #public function to train the ensemble\n",
    "    def fit(self, X_train : np.array, y_train : np.array) -> None:\n",
    "        #initialize residuals\n",
    "        r = np.copy(y_train).astype(float)\n",
    "        #loop through the specified number of iterations in the ensemble\n",
    "        for _ in range(self.n_elements):\n",
    "            #make a copy of the weak learner\n",
    "            model = clone(self.weak_learner)\n",
    "            #fit the weak learner on the current dataset\n",
    "            model.fit(X_train,r)\n",
    "            #update the residuals\n",
    "            r -= self.learning_rate*model.predict(X_train)\n",
    "            #append resulting model\n",
    "            self.f.append(model)\n",
    "            #append current mean residual\n",
    "            self.residuals.append(np.mean(r))    \n",
    "            \n",
    "    #public function to return residuals\n",
    "    def get_residuals(self) -> List:\n",
    "        return(self.residuals)\n",
    "    \n",
    "    #public function to generate predictions\n",
    "    def predict(self, X_test : np.array) -> np.array:\n",
    "        #initialize output\n",
    "        y_pred = np.zeros((X_test.shape[0]))\n",
    "        #traverse ensemble to generate predictions\n",
    "        for model in self.f:\n",
    "            y_pred += self.learning_rate*model.predict(X_test)\n",
    "        #return predictions\n",
    "        return(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradBoostModel = GradientBoostTreeRegressor(n_elements=1000, learning_rate=0.1)\n",
    "\n",
    "#fit the model\n",
    "gradBoostModel.fit(Xtrain.values,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Regression - Mean Squared error: 525.8333\n",
      "Gradient Boost Regression - R-Squared: 0.9506\n"
     ]
    }
   ],
   "source": [
    "Ypred_train = gradBoostModel.predict(Xtrain.values)\n",
    "\n",
    "print('Gradient Boost Regression - Mean Squared error:', round(mean_squared_error(Ytrain,Ypred_train),4))\n",
    "print('Gradient Boost Regression - R-Squared:', round(r2_score(Ytrain,Ypred_train),4))\n",
    "print('Random Forest - precision_score',round(precision_score(Ytrain,Ypred),4))\n",
    "print('Random Forest - recall_score',round(recall_score(Ytrain,Ypred),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training dataset accuracy output for the random forest model, we can find that the mean squared error is 525.8333 and the R - squared value is 0.9506. This means our model has 95.06% accuracy on the training dataset.This is a very high probability rate. This means our model performance is really perfect on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  b) Predict the models using the test dataset, and provide the performance metrics. Compare the four models' performance metrics, and explain at least four findings on each of the models. Do not repeat the code to fit the model. (4x3=12 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Regression - Mean Squared error: 4686.9976\n",
      "Gradient Boost Regression - R-Squared: 0.5316\n",
      "Gradient Boost Regression - Mean Absolute error: 34.5779\n",
      "Gradient Boost Regression - Explained variance score: 0.5319\n"
     ]
    }
   ],
   "source": [
    "Ypred = gradBoostModel.predict(Xtest.values)\n",
    "\n",
    "print('Gradient Boost Regression - Mean Squared error:', round(mean_squared_error(Ytest,Ypred),4))\n",
    "print('Gradient Boost Regression - R-Squared:', round(r2_score(Ytest,Ypred),4))\n",
    "print('Gradient Boost Regression - Mean Absolute error:', round(mean_absolute_error(Ytest,Ypred),4))\n",
    "print('Gradient Boost Regression - Explained variance score:',round(explained_variance_score(Ytest,Ypred),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:\n",
    "\n",
    "From the testing dataset accuracy output for the Gradient Boost Regression model, we can find that:\n",
    "\n",
    "• The mean squared error is 4686.9976, which is kind of large. This means our mean squared error regression loss is 4686.9976.\n",
    "\n",
    "• R - squared value is 0.5316. This means we have 53.16% accurancy on the testing dataset. This is a low rate. This means our model performance is not working very well on the testing dataset because the accuracy is just a little bit above 50%. Compared to the training dataset which has 95.06% accuracy, our testing dataset accuracy is extremely low. Therefore, our model has a significant overfitting because the training dataset accuracy is greatly higher than the testing dataset accuracy.\n",
    "\n",
    "• The mean absolute error is 34.5779, which means mean absolute error regression loss of this model is 34.5779, kind of large.\n",
    "\n",
    "• The explained variance regression score of this function is 0.5319, which is small. Our model is not good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Do you see any bias and variance issues? How do you interpret each model output? (4x3=12 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Gradient Boost model, we can see that for the training dataset, the MSE is 525.8333, which is significanly less than the MSE of the testing dataset which is 4686.9976. And the R-squared for the predicting training dataset is 0.9506. It's also significantly greater than the testing dataset R-squared which is 0.5316. This means the performance of the random forest model works well on the training dataset and it performs bad in the testing dataset. Therefore, we have an overfitting issue and thus we have a <b>high variance </b> in our model. Variance indicates how much the estimate of the target function will alter if different training data were used. And it measures the inconsistency of different predictions using different training sets. Because that our training dataset works well, but our testing dataset works bad, we have a greatly inconsistency of different predictions using different sets. Therefore, we have a high variance in this model.\n",
    "\n",
    "Bias means the amount that a model's prediction differs from the target value, compared to the training data. Because that our model overall accuracy (95.06% in training dataset) works perfect, we could say we have a <b>low bias</b>.\n",
    "\n",
    "Therefore, we have a high variance and low bias in the Gradient Boost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Write a function to find important features in each model? Why is it an important feature of the model? Explain with some statistical evidence. (6 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_after_permutation(model, X, y, curr_feat):\n",
    "    \"\"\" return the score of model when curr_feat is permuted \"\"\"\n",
    "\n",
    "    X_permuted = X.copy()\n",
    "    col_idx = list(X.columns).index(curr_feat)\n",
    "    # permute one column\n",
    "    X_permuted.iloc[:, col_idx] = np.random.permutation(\n",
    "        X_permuted[curr_feat].values)\n",
    "    y_pred = model.predict(X_permuted.values)\n",
    "\n",
    "    permuted_score = r2_score(y, y_pred)\n",
    "    return permuted_score\n",
    "\n",
    "\n",
    "def get_feature_importance(model, X, y, curr_feat):\n",
    "    \"\"\" compare the score when curr_feat is permuted \"\"\"\n",
    "    y_pred = model.predict(X.values)\n",
    "\n",
    "    baseline_score_train = r2_score(y, y_pred)\n",
    "    permuted_score_train = get_score_after_permutation(model, X, y, curr_feat)\n",
    "\n",
    "    # feature importance is the difference between the two scores\n",
    "    feature_importance = baseline_score_train - permuted_score_train\n",
    "    return feature_importance\n",
    "    \n",
    "def permutation_importance(model, X, y, n_repeats=10):\n",
    "    \"\"\"Calculate importance score for each feature.\"\"\"\n",
    "\n",
    "    importances = []\n",
    "    for curr_feat in X.columns:\n",
    "        list_feature_importance = []\n",
    "        for n_round in range(n_repeats):\n",
    "            list_feature_importance.append(\n",
    "                get_feature_importance(model, X, y, curr_feat))\n",
    "\n",
    "        importances.append(list_feature_importance)\n",
    "\n",
    "    return {'importances_mean': np.mean(importances, axis=1),\n",
    "            'importances_std': np.std(importances, axis=1),\n",
    "            'importances': importances}\n",
    "\n",
    "def plot_feature_importances(perm_importance_result, feat_name):\n",
    "    \"\"\" bar plot the feature importance \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    indices = perm_importance_result['importances_mean'].argsort()\n",
    "    plt.barh(range(len(indices)),\n",
    "             perm_importance_result['importances_mean'][indices],\n",
    "             xerr=perm_importance_result['importances_std'][indices])\n",
    "\n",
    "    ax.set_yticks(range(len(indices)))\n",
    "    _ = ax.set_yticklabels(feat_name[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>T5</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>-0.47908</td>\n",
       "      <td>-1.118645</td>\n",
       "      <td>1.316914</td>\n",
       "      <td>-0.520411</td>\n",
       "      <td>1.147399</td>\n",
       "      <td>-0.935970</td>\n",
       "      <td>1.550932</td>\n",
       "      <td>-0.859265</td>\n",
       "      <td>1.767501</td>\n",
       "      <td>-1.128170</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.151142</td>\n",
       "      <td>1.721736</td>\n",
       "      <td>0.543199</td>\n",
       "      <td>0.861898</td>\n",
       "      <td>0.385860</td>\n",
       "      <td>1.411679</td>\n",
       "      <td>-1.257445</td>\n",
       "      <td>0.998749</td>\n",
       "      <td>0.983207</td>\n",
       "      <td>0.983207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19730</th>\n",
       "      <td>-0.47908</td>\n",
       "      <td>2.415962</td>\n",
       "      <td>1.583299</td>\n",
       "      <td>2.530318</td>\n",
       "      <td>0.394449</td>\n",
       "      <td>2.458745</td>\n",
       "      <td>0.590209</td>\n",
       "      <td>1.882026</td>\n",
       "      <td>1.511812</td>\n",
       "      <td>1.955947</td>\n",
       "      <td>...</td>\n",
       "      <td>1.843571</td>\n",
       "      <td>1.261649</td>\n",
       "      <td>2.881489</td>\n",
       "      <td>-0.043599</td>\n",
       "      <td>-1.616282</td>\n",
       "      <td>-0.288198</td>\n",
       "      <td>-1.243314</td>\n",
       "      <td>2.282163</td>\n",
       "      <td>1.249203</td>\n",
       "      <td>1.249203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3288</th>\n",
       "      <td>-0.47908</td>\n",
       "      <td>0.506486</td>\n",
       "      <td>1.048015</td>\n",
       "      <td>0.543620</td>\n",
       "      <td>0.533104</td>\n",
       "      <td>-0.151679</td>\n",
       "      <td>1.632358</td>\n",
       "      <td>0.560332</td>\n",
       "      <td>0.331650</td>\n",
       "      <td>-0.266786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.439691</td>\n",
       "      <td>0.958136</td>\n",
       "      <td>-0.152647</td>\n",
       "      <td>0.632145</td>\n",
       "      <td>-1.057025</td>\n",
       "      <td>1.615664</td>\n",
       "      <td>0.141522</td>\n",
       "      <td>-0.848891</td>\n",
       "      <td>1.177312</td>\n",
       "      <td>1.177312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7730</th>\n",
       "      <td>-0.47908</td>\n",
       "      <td>-1.180910</td>\n",
       "      <td>-0.552810</td>\n",
       "      <td>-1.432438</td>\n",
       "      <td>0.126032</td>\n",
       "      <td>-0.831287</td>\n",
       "      <td>-0.638860</td>\n",
       "      <td>-1.206822</td>\n",
       "      <td>-0.466898</td>\n",
       "      <td>-0.705911</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543927</td>\n",
       "      <td>-0.199708</td>\n",
       "      <td>-0.998945</td>\n",
       "      <td>-1.890633</td>\n",
       "      <td>0.978673</td>\n",
       "      <td>-1.240129</td>\n",
       "      <td>0.876332</td>\n",
       "      <td>-0.602539</td>\n",
       "      <td>-0.853473</td>\n",
       "      <td>-0.853473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8852</th>\n",
       "      <td>-0.47908</td>\n",
       "      <td>-0.676559</td>\n",
       "      <td>-1.248930</td>\n",
       "      <td>-1.478040</td>\n",
       "      <td>-0.154906</td>\n",
       "      <td>-0.985819</td>\n",
       "      <td>-0.689047</td>\n",
       "      <td>-0.663458</td>\n",
       "      <td>-1.042775</td>\n",
       "      <td>-0.754702</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277546</td>\n",
       "      <td>-0.352268</td>\n",
       "      <td>-1.556875</td>\n",
       "      <td>1.722345</td>\n",
       "      <td>0.844451</td>\n",
       "      <td>-0.968148</td>\n",
       "      <td>-0.367193</td>\n",
       "      <td>-1.357488</td>\n",
       "      <td>-0.971982</td>\n",
       "      <td>-0.971982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>-0.47908</td>\n",
       "      <td>0.045721</td>\n",
       "      <td>0.160902</td>\n",
       "      <td>-0.433769</td>\n",
       "      <td>0.655963</td>\n",
       "      <td>0.365088</td>\n",
       "      <td>-0.074512</td>\n",
       "      <td>0.315574</td>\n",
       "      <td>0.175780</td>\n",
       "      <td>0.275344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432251</td>\n",
       "      <td>-0.239053</td>\n",
       "      <td>0.405283</td>\n",
       "      <td>-0.683303</td>\n",
       "      <td>-0.139842</td>\n",
       "      <td>1.071703</td>\n",
       "      <td>0.141522</td>\n",
       "      <td>0.494125</td>\n",
       "      <td>1.415893</td>\n",
       "      <td>1.415893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964</th>\n",
       "      <td>-0.47908</td>\n",
       "      <td>0.437994</td>\n",
       "      <td>0.864561</td>\n",
       "      <td>0.756426</td>\n",
       "      <td>0.404512</td>\n",
       "      <td>0.215541</td>\n",
       "      <td>0.610694</td>\n",
       "      <td>0.212776</td>\n",
       "      <td>1.173197</td>\n",
       "      <td>0.058492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399160</td>\n",
       "      <td>0.659441</td>\n",
       "      <td>1.859663</td>\n",
       "      <td>-0.813947</td>\n",
       "      <td>-0.922803</td>\n",
       "      <td>-0.832158</td>\n",
       "      <td>0.141522</td>\n",
       "      <td>1.702044</td>\n",
       "      <td>0.686157</td>\n",
       "      <td>0.686157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>-0.47908</td>\n",
       "      <td>-0.869583</td>\n",
       "      <td>-1.145894</td>\n",
       "      <td>-0.976425</td>\n",
       "      <td>-0.695485</td>\n",
       "      <td>-1.080532</td>\n",
       "      <td>-0.507759</td>\n",
       "      <td>-1.299830</td>\n",
       "      <td>-0.860798</td>\n",
       "      <td>-0.592063</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.792107</td>\n",
       "      <td>-0.502418</td>\n",
       "      <td>-1.594488</td>\n",
       "      <td>-0.084144</td>\n",
       "      <td>0.486526</td>\n",
       "      <td>-0.152208</td>\n",
       "      <td>2.063334</td>\n",
       "      <td>-1.595893</td>\n",
       "      <td>-0.408152</td>\n",
       "      <td>-0.408152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>-0.47908</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>-1.181077</td>\n",
       "      <td>0.042309</td>\n",
       "      <td>-1.303390</td>\n",
       "      <td>-0.532193</td>\n",
       "      <td>-0.719774</td>\n",
       "      <td>0.119768</td>\n",
       "      <td>-0.774800</td>\n",
       "      <td>-0.836022</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.489327</td>\n",
       "      <td>-0.020652</td>\n",
       "      <td>-1.005214</td>\n",
       "      <td>1.285364</td>\n",
       "      <td>-1.034654</td>\n",
       "      <td>-1.104138</td>\n",
       "      <td>0.141522</td>\n",
       "      <td>-1.866086</td>\n",
       "      <td>-1.651283</td>\n",
       "      <td>-1.651283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>-0.47908</td>\n",
       "      <td>-0.226172</td>\n",
       "      <td>-0.635741</td>\n",
       "      <td>-0.205762</td>\n",
       "      <td>-0.455500</td>\n",
       "      <td>0.215541</td>\n",
       "      <td>-0.815027</td>\n",
       "      <td>-0.565555</td>\n",
       "      <td>0.093621</td>\n",
       "      <td>-0.212573</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047565</td>\n",
       "      <td>-0.068828</td>\n",
       "      <td>-0.143243</td>\n",
       "      <td>0.422664</td>\n",
       "      <td>0.385860</td>\n",
       "      <td>-1.240129</td>\n",
       "      <td>0.141522</td>\n",
       "      <td>0.140490</td>\n",
       "      <td>-0.519582</td>\n",
       "      <td>-0.519582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15788 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lights        T1      RH_1        T2      RH_2        T3      RH_3  \\\n",
       "2133  -0.47908 -1.118645  1.316914 -0.520411  1.147399 -0.935970  1.550932   \n",
       "19730 -0.47908  2.415962  1.583299  2.530318  0.394449  2.458745  0.590209   \n",
       "3288  -0.47908  0.506486  1.048015  0.543620  0.533104 -0.151679  1.632358   \n",
       "7730  -0.47908 -1.180910 -0.552810 -1.432438  0.126032 -0.831287 -0.638860   \n",
       "8852  -0.47908 -0.676559 -1.248930 -1.478040 -0.154906 -0.985819 -0.689047   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "11284 -0.47908  0.045721  0.160902 -0.433769  0.655963  0.365088 -0.074512   \n",
       "11964 -0.47908  0.437994  0.864561  0.756426  0.404512  0.215541  0.610694   \n",
       "5390  -0.47908 -0.869583 -1.145894 -0.976425 -0.695485 -1.080532 -0.507759   \n",
       "860   -0.47908  0.064400 -1.181077  0.042309 -1.303390 -0.532193 -0.719774   \n",
       "15795 -0.47908 -0.226172 -0.635741 -0.205762 -0.455500  0.215541 -0.815027   \n",
       "\n",
       "             T4      RH_4        T5  ...        T9      RH_9     T_out  \\\n",
       "2133  -0.859265  1.767501 -1.128170  ... -1.151142  1.721736  0.543199   \n",
       "19730  1.882026  1.511812  1.955947  ...  1.843571  1.261649  2.881489   \n",
       "3288   0.560332  0.331650 -0.266786  ... -0.439691  0.958136 -0.152647   \n",
       "7730  -1.206822 -0.466898 -0.705911  ... -0.543927 -0.199708 -0.998945   \n",
       "8852  -0.663458 -1.042775 -0.754702  ... -0.277546 -0.352268 -1.556875   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "11284  0.315574  0.175780  0.275344  ...  0.432251 -0.239053  0.405283   \n",
       "11964  0.212776  1.173197  0.058492  ...  0.399160  0.659441  1.859663   \n",
       "5390  -1.299830 -0.860798 -0.592063  ... -0.792107 -0.502418 -1.594488   \n",
       "860    0.119768 -0.774800 -0.836022  ... -0.489327 -0.020652 -1.005214   \n",
       "15795 -0.565555  0.093621 -0.212573  ... -0.047565 -0.068828 -0.143243   \n",
       "\n",
       "       Press_mm_hg    RH_out  Windspeed  Visibility  Tdewpoint       rv1  \\\n",
       "2133      0.861898  0.385860   1.411679   -1.257445   0.998749  0.983207   \n",
       "19730    -0.043599 -1.616282  -0.288198   -1.243314   2.282163  1.249203   \n",
       "3288      0.632145 -1.057025   1.615664    0.141522  -0.848891  1.177312   \n",
       "7730     -1.890633  0.978673  -1.240129    0.876332  -0.602539 -0.853473   \n",
       "8852      1.722345  0.844451  -0.968148   -0.367193  -1.357488 -0.971982   \n",
       "...            ...       ...        ...         ...        ...       ...   \n",
       "11284    -0.683303 -0.139842   1.071703    0.141522   0.494125  1.415893   \n",
       "11964    -0.813947 -0.922803  -0.832158    0.141522   1.702044  0.686157   \n",
       "5390     -0.084144  0.486526  -0.152208    2.063334  -1.595893 -0.408152   \n",
       "860       1.285364 -1.034654  -1.104138    0.141522  -1.866086 -1.651283   \n",
       "15795     0.422664  0.385860  -1.240129    0.141522   0.140490 -0.519582   \n",
       "\n",
       "            rv2  \n",
       "2133   0.983207  \n",
       "19730  1.249203  \n",
       "3288   1.177312  \n",
       "7730  -0.853473  \n",
       "8852  -0.971982  \n",
       "...         ...  \n",
       "11284  1.415893  \n",
       "11964  0.686157  \n",
       "5390  -0.408152  \n",
       "860   -1.651283  \n",
       "15795 -0.519582  \n",
       "\n",
       "[15788 rows x 27 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.columns = dfX.columns\n",
    "Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "perm_importance_result_train = permutation_importance(gradBoostModel, Xtrain, Ytrain, n_repeats=5)\n",
    "plot_feature_importances(perm_importance_result_train, Xtrain.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Grdient Boost model output, we can see that the importance features of this model are 'T6', 'RH_6','T4','RH_2' and 'RH_8'. This is because they takes a great porpotion in the importance feature chart. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
